# Cache System Documentation

## Overview

The podcast generator now includes a **persistent file-based caching system** that stores YouTube search results and video transcripts. This significantly improves performance and reduces API calls, especially when dealing with rate limits.

## Features

‚úÖ **Persistent Storage**: Cache survives server restarts  
‚úÖ **Hash-based Keys**: Efficient lookup using SHA256 hashes  
‚úÖ **Thread-Safe**: Uses locks to prevent race conditions  
‚úÖ **Separate Storage**: Search results and transcripts stored separately  
‚úÖ **JSON Format**: Human-readable cache files  
‚úÖ **Management API**: Endpoints to view and clear cache  

## How It Works

### Search Results Caching

When you search for podcasts:
1. System checks cache first using query + max_results as key
2. If found (cache HIT), returns cached results immediately
3. If not found (cache MISS), fetches from YouTube API and caches the result

**Cache Key**: `"{query}:{max_results}"`  
**Example**: `"AI podcast:10"` ‚Üí `4f014851100a...` (SHA256 hash)

### Transcript Caching

When you fetch a transcript:
1. System checks cache first using video_id as key
2. If found (cache HIT), returns cached transcript immediately
3. If not found (cache MISS), fetches from YouTube and caches it
4. Even fallback transcripts are cached to avoid repeated failures

**Cache Key**: `"{video_id}"`  
**Example**: `"test123"` ‚Üí `ecd71870d19...` (SHA256 hash)

## File Structure

```
backend/
‚îî‚îÄ‚îÄ cache/
    ‚îú‚îÄ‚îÄ search_hashmap.json          # Maps search queries to file hashes
    ‚îú‚îÄ‚îÄ transcript_hashmap.json      # Maps video IDs to file hashes
    ‚îú‚îÄ‚îÄ searches/
    ‚îÇ   ‚îî‚îÄ‚îÄ {hash}.json              # Cached search results
    ‚îî‚îÄ‚îÄ transcripts/
        ‚îî‚îÄ‚îÄ {hash}.json              # Cached transcripts
```

### Example Cache Files

**search_hashmap.json:**
```json
{
  "AI podcast:10": "4f014851100a9e8d9c1a6812b7ee6bcce2d6f8b5bf036d7ea0e5f1f66f0a1d88"
}
```

**searches/{hash}.json:**
```json
{
  "query": "AI podcast",
  "max_results": 10,
  "cached_at": "2024-12-13T10:30:00",
  "results": [
    {
      "video_id": "abc123",
      "title": "AI Podcast Episode",
      "channel_name": "Tech Talk",
      ...
    }
  ]
}
```

**transcript_hashmap.json:**
```json
{
  "abc123": "ecd71870d1963316a97e3ac3408c9835ad8cf0f3c1bc703527c30265534f75ae"
}
```

**transcripts/{hash}.json:**
```json
{
  "video_id": "abc123",
  "cached_at": "2024-12-13T10:31:00",
  "transcript": "Welcome to this podcast episode..."
}
```

## API Endpoints

### Get Cache Statistics

```bash
GET /api/v1/cache/stats
```

**Response:**
```json
{
  "success": true,
  "stats": {
    "search_entries": 15,
    "transcript_entries": 42,
    "cache_dir": "/path/to/cache",
    "search_cache_size_mb": 0.25,
    "transcript_cache_size_mb": 1.5
  }
}
```

### Clear Search Cache

```bash
DELETE /api/v1/cache/clear/search
```

Removes all cached search results.

### Clear Transcript Cache

```bash
DELETE /api/v1/cache/clear/transcripts
```

Removes all cached transcripts.

### Clear All Cache

```bash
DELETE /api/v1/cache/clear/all
```

Removes all cache data (both searches and transcripts).

## Usage Examples

### Check Cache Status

```bash
curl http://localhost:8000/api/v1/cache/stats
```

### Clear Old Cache

```bash
# Clear only search cache
curl -X DELETE http://localhost:8000/api/v1/cache/clear/search

# Clear everything
curl -X DELETE http://localhost:8000/api/v1/cache/clear/all
```

## Benefits

### 1. **Speed** üöÄ
- First search: ~2-3 seconds (API call)
- Cached search: ~50ms (from disk)
- **40-60x faster** for repeated queries

### 2. **Cost Reduction** üí∞
- Reduces YouTube API quota usage
- Reduces transcript API calls
- Especially important for rate-limited APIs

### 3. **Reliability** üõ°Ô∏è
- Works even when YouTube blocks requests
- Fallback transcripts are also cached
- No data loss on server restart

### 4. **Development** üîß
- Faster iteration during development
- No need to re-fetch same videos
- Test with real data offline

## Configuration

The cache is automatically initialized when the server starts. The cache directory is created at:

```
backend/cache/
```

You can customize the cache location in `backend/app/core/cache.py`:

```python
# Change cache directory
cache = FileCache(cache_dir="custom_cache_path")
```

## Persistence

The cache **automatically persists** across server restarts:

1. On server start, the cache system loads existing hashmaps from disk
2. Cache files are immediately available
3. Logs show: `"Loaded N search cache entries"` and `"Loaded M transcript cache entries"`

**Test it yourself:**
```bash
# Make a search request
curl -X POST http://localhost:8000/api/v1/discover \
  -H "Content-Type: application/json" \
  -d '{"topic": "AI", "max_results": 5}'

# Restart server
pkill -f uvicorn
uvicorn app.main:app --reload

# Make same request - it will be instant!
curl -X POST http://localhost:8000/api/v1/discover \
  -H "Content-Type: application/json" \
  -d '{"topic": "AI", "max_results": 5}'
```

## Monitoring

The cache system logs important events:

- **Cache HIT**: `"Cache HIT for search: 'query'"`
- **Cache MISS**: `"Cache MISS for search: 'query'"`
- **Startup**: `"Loaded N search cache entries"`
- **Save**: `"Cached search results for: 'query'"`

## Git Ignore

By default, the cache directory is **ignored** by Git:

```gitignore
# .gitignore
backend/cache/
```

If you want to commit cache files (e.g., for demos), remove this line from `.gitignore`.

## Troubleshooting

### Cache Not Working?

1. **Check cache stats:**
   ```bash
   curl http://localhost:8000/api/v1/cache/stats
   ```

2. **Check logs** for "Cache HIT/MISS" messages

3. **Verify cache directory exists:**
   ```bash
   ls -la backend/cache/
   ```

### Clear Cache

If you want fresh data:
```bash
curl -X DELETE http://localhost:8000/api/v1/cache/clear/all
```

### Cache Files Too Large?

Monitor cache size via the stats endpoint. Clear old cache periodically:
```bash
# Check size
curl http://localhost:8000/api/v1/cache/stats

# Clear if too large
curl -X DELETE http://localhost:8000/api/v1/cache/clear/all
```

## Implementation Details

### Thread Safety

The cache uses Python's `threading.Lock()` to ensure thread-safe operations:
- Separate locks for search and transcript operations
- Prevents race conditions during concurrent requests
- Safe for production use with multiple workers

### Hash Function

Uses SHA256 for cache keys:
- Consistent hashing across restarts
- No collision risk
- Human-readable in hashmaps

### Performance

- **Lookup**: O(1) - hash table lookup
- **Storage**: O(1) - direct file write
- **Memory**: Minimal - only hashmaps in RAM
- **Disk**: JSON files are human-readable and editable

## Future Enhancements

Possible improvements:
- [ ] Add TTL (time-to-live) for cache expiration
- [ ] Implement LRU eviction policy
- [ ] Add cache warming on startup
- [ ] Support Redis/Memcached backends
- [ ] Add cache compression for large transcripts
- [ ] Implement cache versioning

---

**Cache system is production-ready and enabled by default!** üéâ

